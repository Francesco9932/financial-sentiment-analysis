{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google colab commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u74sT_sz2-Lm",
        "outputId": "e990ba55-0532-47df-da2e-e72ab1d662aa"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Francesco9932/financial-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAah7XGk3fcD",
        "outputId": "3d2df4ac-017c-4f41-966b-5d799b803a2b"
      },
      "outputs": [],
      "source": [
        "# %cd financial-sentiment-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !unzip glove.6B.200d.txt.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9UHwVbQ72Yjh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/francesco/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/home/francesco/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-04-23 19:23:38.217970: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-23 19:23:38.259345: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-23 19:23:38.820114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# pre-processing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertModel\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9fcdftTO2Yjk",
        "outputId": "6611b847-77f1-4f8b-c165-a8e9eb9a4c7a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment                                           headline\n",
              "0   neutral  According to Gran , the company has no plans t...\n",
              "1   neutral  Technopolis plans to develop in stages an area...\n",
              "2  negative  The international electronic industry company ...\n",
              "3  positive  With the new production plant the company woul...\n",
              "4  positive  According to the company 's updated strategy f..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('all-data.csv', encoding=\"latin-1\",\n",
        "                 names=['sentiment', 'headline'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "pfrwFBDm2Yjk",
        "outputId": "1e202535-e00d-4169-ea43-ef02e97edf0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='sentiment'>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df['sentiment'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBiWrbvb2Yjl",
        "outputId": "050ed99d-52db-4642-e8b0-5fdcb8599a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original shape:  (4846, 2)\n",
            "after drop duplicates shape:  (4840, 2)\n",
            "after drop null shape:  (4840, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"original shape: \", df.shape)\n",
        "df = df.drop_duplicates()\n",
        "print(\"after drop duplicates shape: \", df.shape)\n",
        "dd_dn = df.dropna()\n",
        "print(\"after drop null shape: \", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Ej9YDwzj2Yjl",
        "outputId": "ab2f006e-b388-4aa0-86e3-c71d47997abe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['headline'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def headline_to_words(headline):\n",
        "    ''' Convert headline text into a sequence of words '''\n",
        "    \n",
        "    # convert to lowercase\n",
        "    text = headline.lower()\n",
        "    # remove non letters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # tokenize\n",
        "    words = text.split()\n",
        "    # remove stopwords\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "\n",
        "    ## apply stemming\n",
        "    # words = [PorterStemmer().stem(w) for w in words]\n",
        "    ## return list\n",
        "\n",
        "    return  ' '.join(map(str, words))\n",
        "\n",
        "#print(\"\\nOriginal headline ->\", df['headline'][0])\n",
        "\n",
        "#df['headline'] = df['headline'].apply(headline_to_words)\n",
        "\n",
        "#print(\"\\nProcessed headline ->\", df['headline'][0])\n",
        "\n",
        "# not used nltk preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = df['headline'].to_list()\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "le.fit(df['sentiment'])\n",
        "df['sentiment'] = le.transform(df['sentiment'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer for other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpP0FaOE2Yjl",
        "outputId": "63ef8ae1-f4c7-4c6b-c874-5590d0a8a91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab length: 10123\n",
            "Maximum sequence length: 71\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(lower=True)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Vocab length:\", len(word_index) + 1)\n",
        "\n",
        "max_seq_length = np.max(list(map(lambda x: len(x), sequences)))\n",
        "print(\"Maximum sequence length:\", max_seq_length)\n",
        "sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vFJ-Xwp52Yjn"
      },
      "outputs": [],
      "source": [
        "train_sequences, test_sequences, y_train, y_test = train_test_split(sequences, df['sentiment'], train_size=0.7, shuffle=True, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmt1g5NA2Yjn",
        "outputId": "c8375944-f65b-43d4-c51a-3354c24475e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set -> (3388, 71) (3388,)\n",
            "Test Set -> (1452, 71) (1452,)\n"
          ]
        }
      ],
      "source": [
        "print('Train Set ->', train_sequences.shape, y_train.shape)\n",
        "print('Test Set ->', test_sequences.shape, y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global Vectors for Word Representation (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 10123\n",
        "embedding_size = 200\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('glove.6B.200d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_size))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10124, 200)\n"
          ]
        }
      ],
      "source": [
        "print(embeddings_matrix.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN1d Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 71)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 71, 200)      2024800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 70, 200)      80200       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 69, 200)      120200      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 68, 200)      160200      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 67, 200)      200200      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 66, 200)      240200      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 200)         0           ['conv1d[0][0]']                 \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 200)         0           ['conv1d_1[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 200)         0           ['conv1d_2[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 200)         0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 200)         0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1000)         0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]', \n",
            "                                                                  'global_max_pooling1d_2[0][0]', \n",
            "                                                                  'global_max_pooling1d_3[0][0]', \n",
            "                                                                  'global_max_pooling1d_4[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1000)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          128128      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            387         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,954,315\n",
            "Trainable params: 2,954,315\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.keras.Input(shape=(train_sequences.shape[1],))\n",
        "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
        "                              output_dim=embedding_size,\n",
        "                              input_length=train_sequences.shape[1],\n",
        "                              weights=[embeddings_matrix])(inputs)\n",
        "\n",
        "convs = []\n",
        "filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "for filter_size in filter_sizes:\n",
        "    l_conv = tf.keras.layers.Conv1D(filters=200, \n",
        "                        kernel_size=filter_size, \n",
        "                        activation='relu')(x)\n",
        "    l_pool = tf.keras.layers.GlobalMaxPooling1D()(l_conv)\n",
        "    convs.append(l_pool)\n",
        "\n",
        "l_merge = tf.keras.layers.concatenate(convs, axis=1)\n",
        "x = tf.keras.layers.Dropout(0.1)(l_merge)  \n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.8990 - accuracy: 0.6150\n",
            "Epoch 1: val_accuracy improved from -inf to 0.66962, saving model to ./best_model/best_model_cnn1d.h5\n",
            "96/96 [==============================] - 5s 47ms/step - loss: 0.8990 - accuracy: 0.6150 - val_loss: 0.7562 - val_accuracy: 0.6696\n",
            "Epoch 2/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.5297 - accuracy: 0.7819\n",
            "Epoch 2: val_accuracy improved from 0.66962 to 0.73746, saving model to ./best_model/best_model_cnn1d.h5\n",
            "96/96 [==============================] - 4s 47ms/step - loss: 0.5292 - accuracy: 0.7822 - val_loss: 0.6001 - val_accuracy: 0.7375\n",
            "Epoch 3/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9109\n",
            "Epoch 3: val_accuracy improved from 0.73746 to 0.76991, saving model to ./best_model/best_model_cnn1d.h5\n",
            "96/96 [==============================] - 4s 47ms/step - loss: 0.2587 - accuracy: 0.9108 - val_loss: 0.5691 - val_accuracy: 0.7699\n",
            "Epoch 4/100\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9711\n",
            "Epoch 4: val_accuracy improved from 0.76991 to 0.78466, saving model to ./best_model/best_model_cnn1d.h5\n",
            "96/96 [==============================] - 5s 48ms/step - loss: 0.0961 - accuracy: 0.9711 - val_loss: 0.6477 - val_accuracy: 0.7847\n",
            "Epoch 5/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0513 - accuracy: 0.9888\n",
            "Epoch 5: val_accuracy did not improve from 0.78466\n",
            "96/96 [==============================] - 4s 46ms/step - loss: 0.0513 - accuracy: 0.9888 - val_loss: 0.7153 - val_accuracy: 0.7758\n",
            "Epoch 6/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9865\n",
            "Epoch 6: val_accuracy did not improve from 0.78466\n",
            "96/96 [==============================] - 5s 49ms/step - loss: 0.0461 - accuracy: 0.9866 - val_loss: 0.8330 - val_accuracy: 0.7434\n",
            "Epoch 7/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0338 - accuracy: 0.9908\n",
            "Epoch 7: val_accuracy did not improve from 0.78466\n",
            "96/96 [==============================] - 5s 51ms/step - loss: 0.0344 - accuracy: 0.9905 - val_loss: 0.9951 - val_accuracy: 0.7345\n",
            "Epoch 8/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0478 - accuracy: 0.9868\n",
            "Epoch 8: val_accuracy did not improve from 0.78466\n",
            "96/96 [==============================] - 5s 48ms/step - loss: 0.0476 - accuracy: 0.9869 - val_loss: 0.9785 - val_accuracy: 0.7581\n",
            "Epoch 8: early stopping\n"
          ]
        }
      ],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('./best_model/best_model_cnn1d.h5', \n",
        "                     monitor='val_accuracy', mode='max', verbose=1, \n",
        "                     save_best_only=True)\n",
        "\n",
        "history = model.fit(train_sequences,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=100, \n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es, mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Losss: 0.58487\n",
            "Test accuracy: 78.92562%\n"
          ]
        }
      ],
      "source": [
        "saved_model = load_model('./best_model/best_model_cnn1d.h5')\n",
        "\n",
        "results = saved_model.evaluate(test_sequences, y_test, verbose =  0)\n",
        "\n",
        "print(\"Test Losss: {:.5f}\".format(results[0]))\n",
        "print(\"Test accuracy: {:.5f}%\".format(results[1]  * 100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "mHXxH7wo2Yjn",
        "outputId": "df828a91-b0bc-4f0b-e40d-79dde7835e7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-23 19:24:28.711543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:24:28.712665: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:24:28.713405: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.keras.Input(shape=(train_sequences.shape[1],))\n",
        "x = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embedding_size,\n",
        "                              input_length=train_sequences.shape[1])(inputs)\n",
        "x = tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh')(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-23 19:24:28.902359: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:24:28.904064: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:24:28.905233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2023-04-23 19:24:29.384098: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:24:29.385427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:24:29.386517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95/96 [============================>.] - ETA: 0s - loss: 0.8157 - accuracy: 0.6454"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-23 19:24:37.543487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:24:37.544843: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:24:37.545736: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.67552, saving model to ./best_model/best_model_lstm.h5\n",
            "96/96 [==============================] - 9s 83ms/step - loss: 0.8158 - accuracy: 0.6451 - val_loss: 0.7150 - val_accuracy: 0.6755\n",
            "Epoch 2/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8352\n",
            "Epoch 2: val_accuracy improved from 0.67552 to 0.71386, saving model to ./best_model/best_model_lstm.h5\n",
            "96/96 [==============================] - 8s 80ms/step - loss: 0.4204 - accuracy: 0.8354 - val_loss: 0.7400 - val_accuracy: 0.7139\n",
            "Epoch 3/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9579\n",
            "Epoch 3: val_accuracy did not improve from 0.71386\n",
            "96/96 [==============================] - 8s 80ms/step - loss: 0.1280 - accuracy: 0.9580 - val_loss: 1.0367 - val_accuracy: 0.7109\n",
            "Epoch 4/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9842\n",
            "Epoch 4: val_accuracy improved from 0.71386 to 0.72271, saving model to ./best_model/best_model_lstm.h5\n",
            "96/96 [==============================] - 8s 81ms/step - loss: 0.0583 - accuracy: 0.9843 - val_loss: 1.0883 - val_accuracy: 0.7227\n",
            "Epoch 5/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9944\n",
            "Epoch 5: val_accuracy did not improve from 0.72271\n",
            "96/96 [==============================] - 8s 81ms/step - loss: 0.0252 - accuracy: 0.9944 - val_loss: 1.1229 - val_accuracy: 0.7080\n",
            "Epoch 6/100\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9961\n",
            "Epoch 6: val_accuracy improved from 0.72271 to 0.73451, saving model to ./best_model/best_model_lstm.h5\n",
            "96/96 [==============================] - 8s 87ms/step - loss: 0.0183 - accuracy: 0.9957 - val_loss: 1.5575 - val_accuracy: 0.7345\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('./best_model/best_model_lstm.h5', \n",
        "                     monitor='val_accuracy', mode='max', verbose=1, \n",
        "                     save_best_only=True)\n",
        "\n",
        "history = model.fit(train_sequences,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=100, \n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es, mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "syZ4FiJY2Yjn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-23 19:25:17.397850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:25:17.399053: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:25:17.400329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2023-04-23 19:25:17.686914: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2023-04-23 19:25:17.688074: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2023-04-23 19:25:17.688784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Losss: 1.43991\n",
            "Test accuracy: 74.31129%\n"
          ]
        }
      ],
      "source": [
        "saved_model = load_model('./best_model/best_model_lstm.h5')\n",
        "\n",
        "results = saved_model.evaluate(test_sequences, y_test, verbose =  0)\n",
        "\n",
        "print(\"Test Losss: {:.5f}\".format(results[0]))\n",
        "print(\"Test accuracy: {:.5f}%\".format(results[1]  * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT Finetune (finBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertModel.\n",
            "\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at ProsusAI/finbert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenir = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "bert = TFBertModel.from_pretrained('ProsusAI/finbert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_max_sentence_len(sentences):\n",
        "    max_len = 0\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "\n",
        "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "        # Update the maximum sentence length.\n",
        "        max_len = max(max_len, len(input_ids))\n",
        "\n",
        "    return max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  120\n",
            "\n",
            "Train split shape:  (3388, 120)\n",
            "\n",
            "Test split shape:  (1452, 120)\n"
          ]
        }
      ],
      "source": [
        "train_sentences, test_sentences, labels_train, labels_test = train_test_split(\n",
        "    sentences, df['sentiment'], train_size=0.7, shuffle=True, random_state=1)\n",
        "\n",
        "max_len = min(calculate_max_sentence_len(train_sentences),\n",
        "              calculate_max_sentence_len(test_sentences))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "X_train = tokenizer(\n",
        "    text=train_sentences,\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_len,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids=False,\n",
        "    return_attention_mask=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "X_test = tokenizer(\n",
        "    text=test_sentences,\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_len,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids=False,\n",
        "    return_attention_mask=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nTrain split shape: \", X_train['input_ids'].shape) # the same for attention mask\n",
        "print(\"\\nTest split shape: \", X_test['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 120)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 120)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 120,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          98432       ['global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 128)          0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 32)           4128        ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 3)            99          ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,584,899\n",
            "Trainable params: 102,659\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "embeddings = bert(input_ids, attention_mask=input_mask)[0]  # 0 = last hidden state, 1 = poller_output\n",
        "\n",
        "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
        "out = tf.keras.layers.Dropout(0.1)(out)\n",
        "out = tf.keras.layers.Dense(32, activation='relu')(out)\n",
        "\n",
        "y = tf.keras.layers.Dense(3, activation='softmax')(out)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
        "\n",
        "\n",
        "model.layers[2].trainable = False \n",
        "# check https://stackoverflow.com/questions/60463829/training-tfbertforsequenceclassification-with-custom-x-and-y-data\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=5e-05,  # HF recommendation\n",
        "    epsilon=1e-08,\n",
        "    #decay=0.01,\n",
        "    clipnorm=1.0\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.CategoricalAccuracy('balanced_accuracy')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metric\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/francesco/.local/lib/python3.10/site-packages/keras/backend.py:5561: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - ETA: 0s - loss: 0.8072 - balanced_accuracy: 0.6428\n",
            "Epoch 1: val_balanced_accuracy improved from -inf to 0.78466, saving model to ./best_model/finetuned_finBERT.h5\n",
            "96/96 [==============================] - 364s 4s/step - loss: 0.8072 - balanced_accuracy: 0.6428 - val_loss: 0.5791 - val_balanced_accuracy: 0.7847\n",
            "Epoch 2/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.5312 - balanced_accuracy: 0.7927\n",
            "Epoch 2: val_balanced_accuracy improved from 0.78466 to 0.83776, saving model to ./best_model/finetuned_finBERT.h5\n",
            "96/96 [==============================] - 349s 4s/step - loss: 0.5312 - balanced_accuracy: 0.7927 - val_loss: 0.4272 - val_balanced_accuracy: 0.8378\n",
            "Epoch 3/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.4433 - balanced_accuracy: 0.8268\n",
            "Epoch 3: val_balanced_accuracy improved from 0.83776 to 0.86431, saving model to ./best_model/finetuned_finBERT.h5\n",
            "96/96 [==============================] - 352s 4s/step - loss: 0.4433 - balanced_accuracy: 0.8268 - val_loss: 0.3702 - val_balanced_accuracy: 0.8643\n",
            "Epoch 4/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.4031 - balanced_accuracy: 0.8350\n",
            "Epoch 4: val_balanced_accuracy did not improve from 0.86431\n",
            "96/96 [==============================] - 352s 4s/step - loss: 0.4031 - balanced_accuracy: 0.8350 - val_loss: 0.3518 - val_balanced_accuracy: 0.8614\n",
            "Epoch 5/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.3760 - balanced_accuracy: 0.8541\n",
            "Epoch 5: val_balanced_accuracy did not improve from 0.86431\n",
            "96/96 [==============================] - 352s 4s/step - loss: 0.3760 - balanced_accuracy: 0.8541 - val_loss: 0.3313 - val_balanced_accuracy: 0.8496\n",
            "Epoch 6/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.3613 - balanced_accuracy: 0.8547\n",
            "Epoch 6: val_balanced_accuracy improved from 0.86431 to 0.86726, saving model to ./best_model/finetuned_finBERT.h5\n",
            "96/96 [==============================] - 353s 4s/step - loss: 0.3613 - balanced_accuracy: 0.8547 - val_loss: 0.3156 - val_balanced_accuracy: 0.8673\n",
            "Epoch 7/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.3461 - balanced_accuracy: 0.8649\n",
            "Epoch 7: val_balanced_accuracy did not improve from 0.86726\n",
            "96/96 [==============================] - 351s 4s/step - loss: 0.3461 - balanced_accuracy: 0.8649 - val_loss: 0.3195 - val_balanced_accuracy: 0.8643\n",
            "Epoch 8/10\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.3375 - balanced_accuracy: 0.8547\n",
            "Epoch 8: val_balanced_accuracy did not improve from 0.86726\n",
            "96/96 [==============================] - 350s 4s/step - loss: 0.3375 - balanced_accuracy: 0.8547 - val_loss: 0.3226 - val_balanced_accuracy: 0.8525\n",
            "Epoch 8: early stopping\n"
          ]
        }
      ],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "mc = ModelCheckpoint('./best_model/finetuned_finBERT.h5',\n",
        "                     monitor='val_balanced_accuracy', mode='max', verbose=1,\n",
        "                     save_best_only=True)\n",
        "\n",
        "history = model.fit(x= {'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
        "                    y= tf.keras.utils.to_categorical(labels_train),\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es, mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "46/46 [==============================] - 137s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       171\n",
            "           1       0.88      0.94      0.91       862\n",
            "           2       0.90      0.78      0.83       419\n",
            "\n",
            "    accuracy                           0.88      1452\n",
            "   macro avg       0.88      0.86      0.87      1452\n",
            "weighted avg       0.88      0.88      0.88      1452\n",
            "\n"
          ]
        }
      ],
      "source": [
        "saved_model = load_model('./best_model/finetuned_finBERT.h5', custom_objects={\"TFBertModel\": TFBertModel})\n",
        "\n",
        "# Classification report:\n",
        "predicted = saved_model.predict(\n",
        "    {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']})\n",
        "y_predicted = np.argmax(predicted, axis=1)\n",
        "print(classification_report(labels_test, y_predicted))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
